{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dstorch.data import calc_data_stats, Cutout\n",
    "from dstorch.utils import random_weight_init\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, datasets, transforms\n",
    "from tqdm import tqdm, tqdm_notebook, trange\n",
    "\n",
    "GPU_IDS = '1, 2'\n",
    "# GPU_IDS = '0, 1, 2, 3, 4, 5, 6, 7'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = GPU_IDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "data_path = 'data/'\n",
    "batch_size = 128\n",
    "\n",
    "cifar10_mean_tuple, cifar10_std_tuple = (0.4914, 0.48216, 0.44653), (0.1281, 0.1242, 0.1551)\n",
    "cifar100_mean_tuple, cifar10_std_tuple = (0.5070, 0.4865, 0.4408), (0.1626, 0.1539, 0.1774)\n",
    "\n",
    "train_transformer = transforms.Compose([transforms.RandomCrop(32, padding=4),\n",
    "                                        transforms.RandomHorizontalFlip(),\n",
    "                                        transforms.ToTensor(),\n",
    "                                        transforms.Normalize(cifar10_mean_tuple, cifar10_std_tuple),\n",
    "                                        Cutout(n_holes=1, length=16)\n",
    "                                       ])\n",
    "\n",
    "test_transformer = transforms.Compose([transforms.ToTensor(),\n",
    "                                       transforms.Normalize(cifar10_mean_tuple, cifar10_std_tuple),\n",
    "                                      \n",
    "                                      ])\n",
    "\n",
    "cifar10_training_set = datasets.CIFAR10(root=data_path, train=True, download=True, transform=train_transformer)\n",
    "cifar10_test_set = datasets.CIFAR10(root=data_path, train=False, transform=test_transformer)\n",
    "\n",
    "cifar10_train_loader = torch.utils.data.DataLoader(cifar10_training_set, batch_size=batch_size, shuffle=True)\n",
    "cifar10_test_loader = torch.utils.data.DataLoader(cifar10_test_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# cifar100_training_set = datasets.CIFAR100(root=data_path, train=True, download=True, transform=transformer)\n",
    "# cifar100_test_set = datasets.CIFAR100(root=data_path, train=False, transform=transformer)\n",
    "\n",
    "# cifar100_train_loader = torch.utils.data.DataLoader(cifar100_training_set, batch_size=batch_size, shuffle=True)\n",
    "# cifar100_test_loader = torch.utils.data.DataLoader(cifar100_test_set, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BridgeNet(nn.Module):\n",
    "    def __init__(self, pretrained=False, freeze_features=False, route=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.route = route\n",
    "        self.dropout_fc = 0.1\n",
    "        \n",
    "        # Feature layers\n",
    "        self.feature_layernames = list(models.resnet50(pretrained=pretrained).children())[:-1]\n",
    "#         self.feature_layernames = list(models.resnet34(pretrained=pretrained).children())[:-1]\n",
    "        self.feature = nn.Sequential(*self.feature_layernames)\n",
    "        \n",
    "         # CIFAR10 Classifier\n",
    "        self.cifar10_layer_dict = nn.ModuleDict([\n",
    "            ['fc1', nn.Linear(2048, 512)],\n",
    "            ['relu1', nn.ReLU()],\n",
    "            ['dp1', nn.Dropout(self.dropout_fc)],\n",
    "            ['fc2', nn.Linear(512, 128)],\n",
    "            ['relu2', nn.ReLU()],\n",
    "            ['dp2', nn.Dropout(self.dropout_fc)],\n",
    "            ['logit', nn.Linear(128, 10)],\n",
    "            ['log_softmax', nn.LogSoftmax(dim=1)]\n",
    "        ])\n",
    "        \n",
    "        # CIFAR100 Classifier\n",
    "        self.cifar100_layer_dict = nn.ModuleDict({\n",
    "            'fc1': nn.Linear(512, 256),\n",
    "            'relu1': nn.ReLU(),\n",
    "            'dp1': nn.Dropout(self.dropout_fc),\n",
    "            'fc2': nn.Linear(256, 128),\n",
    "            'relu2': nn.ReLU(),\n",
    "            'dp2': nn.Dropout(self.dropout_fc),\n",
    "            'logit': nn.Linear(128, 100),\n",
    "            'log_softmax': nn.LogSoftmax(dim=1)\n",
    "        })\n",
    "        \n",
    "        if freeze_features:\n",
    "            self.freeze_feature_layers()\n",
    "                \n",
    "    def forward(self, x):    \n",
    "        x = self.feature(x)\n",
    "        \n",
    "        batch_size, num_channels, height, weight = x.shape    \n",
    "        x = x.view(-1, num_channels * height * weight)\n",
    "        \n",
    "        if self.route == 'cifar10':\n",
    "            for layername,  layer in self.cifar10_layer_dict.items():\n",
    "                x = layer(x)\n",
    "#             for layer in self.cifar10_layer_list:\n",
    "#                 x = layer(x)\n",
    "\n",
    "        elif self.route == 'cifar100':\n",
    "            cifar10_hlayer_dict = {}\n",
    "            \n",
    "            h = x\n",
    "            \n",
    "            for layername, layer in self.cifar10_layer_dict.items():\n",
    "                h = layer(h)\n",
    "                cifar10_hlayer_dict[layername] = h\n",
    "                \n",
    "            for layername, layer in self.cifar100_layer_dict.items():\n",
    "                if layername != 'logit' and layername != 'log_softmax':\n",
    "                    x = cifar10_hlayer_dict[layername] + self.cifar10_layer_dict[layername](x)\n",
    "                else:\n",
    "                    x = layer(x)\n",
    "        else:\n",
    "            raise ValueError(\"Route is not set!\")\n",
    "                \n",
    "        return x\n",
    "    \n",
    "    def set_route(self, route):\n",
    "        self.route = route\n",
    "        \n",
    "    def set_training_type(self, ttype):\n",
    "        self.ttype = ttype\n",
    "    \n",
    "    def init_weight(self, modulename):\n",
    "        modulename_dict = {\n",
    "            'feature': self.feature,\n",
    "            'cifar10': self.cifar10_layer_dict,\n",
    "            'cifar100': self.cifar100_layer_dict\n",
    "        }\n",
    "        \n",
    "        random_weight_init(modulename_dict[modulename])\n",
    "        \n",
    "    def freeze_module(self, modulename):\n",
    "        if modulename == 'feature':\n",
    "            for child in self.feature.children():\n",
    "                for param in child.parameters():\n",
    "                    param.requires_grad = False\n",
    "        \n",
    "        elif modulename == 'cifar10':\n",
    "            for child in self.cifar10_layer_dict.children():\n",
    "                for param in child.parameters():\n",
    "                    param.requires_grad = False\n",
    "                \n",
    "    def unfreeze_feature_layers(self):\n",
    "        if modulename == 'feature':\n",
    "            for child in self.feature.children():\n",
    "                for param in child.parameters():\n",
    "                    param.requires_grad = True\n",
    "        \n",
    "        elif modulename == 'cifar10':\n",
    "            for child in self.cifar10_layer_dict.children():\n",
    "                for param in child.parameters():\n",
    "                    param.requires_grad = True\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "def train(model, train_loader, test_loader, loss_func, num_epochs, log_interval=100):\n",
    "    loss_list, acc_list = [], []\n",
    "    loss_test_list, acc_test_list = [], []\n",
    "    cycle = 1\n",
    "    \n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "    \n",
    "        # SGDR Warm start\n",
    "        if epoch == cycle:\n",
    "            optimizer = optim.SGD(model.parameters(), lr=0.05, momentum=0.5, weight_decay=0.0001, nesterov=True)\n",
    "            optim_scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=len(train_loader) * cycle,\n",
    "                                                                   eta_min=0, last_epoch=-1)\n",
    "            cycle = cycle * 2\n",
    "            \n",
    "        for i, (data, target) in enumerate(train_loader):\n",
    "            if torch.cuda.is_available():\n",
    "                model = model.cuda()\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "                \n",
    "            data, target = Variable(data, requires_grad=True), Variable(target)\n",
    "\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            \n",
    "            loss_sum = loss_func(output, target).mean()\n",
    "            \n",
    "            loss_sum.backward()\n",
    "            optimizer.step()\n",
    "            optim_scheduler.step()\n",
    "            \n",
    "            loss_list.append(loss_sum.detach())\n",
    "            \n",
    "            if i % log_interval == 0:\n",
    "                print(\"Epoch: {0}, Iter: {1}, Train loss: {2:.4f}, LR: {3:.6f}\".format(epoch, i, loss_sum, optim_scheduler.get_lr()[0]))\n",
    "            \n",
    "        model.eval()\n",
    "\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        num_rows = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i, (data_test, target_test) in enumerate(test_loader):\n",
    "                if torch.cuda.is_available():\n",
    "                    data_test, target_test = data_test.cuda(), target_test.cuda()\n",
    "\n",
    "                data_test, target_test = Variable(data_test), Variable(target_test)\n",
    "                num_rows += data_test.size(0)\n",
    "                \n",
    "                output = model(data_test)\n",
    "                loss_test = loss_func(output, target_test).mean()\n",
    "\n",
    "                pred = output.data.max(1, keepdim=True)[1]  # get the index of the max log-probability\n",
    "                correct += pred.eq(target_test.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "            test_loss /= num_rows\n",
    "\n",
    "            print(\"=====================================================================\")\n",
    "            print(\"Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n\".format(\n",
    "                test_loss, correct, num_rows, (100. * correct.item()) / num_rows))\n",
    "            print(\"=====================================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Iter: 0, Train loss: 2.9215, LR: 0.050000\n",
      "Epoch: 1, Iter: 100, Train loss: 1.2593, LR: 0.042355\n",
      "Epoch: 1, Iter: 200, Train loss: 1.1131, LR: 0.024096\n",
      "Epoch: 1, Iter: 300, Train loss: 1.1538, LR: 0.006390\n",
      "=====================================================================\n",
      "Test set: Average loss: 0.0000, Accuracy: 7081/10000 (70.81%)\n",
      "\n",
      "=====================================================================\n",
      "Epoch: 2, Iter: 0, Train loss: 1.0239, LR: 0.050000\n",
      "Epoch: 2, Iter: 100, Train loss: 1.1370, LR: 0.048010\n",
      "Epoch: 2, Iter: 200, Train loss: 1.0395, LR: 0.042355\n",
      "Epoch: 2, Iter: 300, Train loss: 1.0352, LR: 0.033937\n",
      "=====================================================================\n",
      "Test set: Average loss: 0.0000, Accuracy: 7630/10000 (76.30%)\n",
      "\n",
      "=====================================================================\n",
      "Epoch: 3, Iter: 0, Train loss: 1.0130, LR: 0.025000\n",
      "Epoch: 3, Iter: 100, Train loss: 0.7994, LR: 0.015225\n",
      "Epoch: 3, Iter: 200, Train loss: 0.7164, LR: 0.007006\n",
      "Epoch: 3, Iter: 300, Train loss: 0.7054, LR: 0.001652\n",
      "=====================================================================\n",
      "Test set: Average loss: 0.0000, Accuracy: 8058/10000 (80.58%)\n",
      "\n",
      "=====================================================================\n",
      "Epoch: 4, Iter: 0, Train loss: 0.7065, LR: 0.050000\n",
      "Epoch: 4, Iter: 100, Train loss: 0.8318, LR: 0.049497\n",
      "Epoch: 4, Iter: 200, Train loss: 1.0508, LR: 0.048010\n",
      "Epoch: 4, Iter: 300, Train loss: 0.9853, LR: 0.045597\n",
      "=====================================================================\n",
      "Test set: Average loss: 0.0000, Accuracy: 7706/10000 (77.06%)\n",
      "\n",
      "=====================================================================\n",
      "Epoch: 5, Iter: 0, Train loss: 0.7022, LR: 0.042678\n",
      "Epoch: 5, Iter: 100, Train loss: 0.9123, LR: 0.038795\n",
      "Epoch: 5, Iter: 200, Train loss: 0.7664, LR: 0.034358\n",
      "Epoch: 5, Iter: 300, Train loss: 0.9571, LR: 0.029544\n",
      "=====================================================================\n",
      "Test set: Average loss: 0.0000, Accuracy: 8247/10000 (82.47%)\n",
      "\n",
      "=====================================================================\n",
      "Epoch: 6, Iter: 0, Train loss: 0.6054, LR: 0.025000\n",
      "Epoch: 6, Iter: 100, Train loss: 0.7372, LR: 0.020012\n",
      "Epoch: 6, Iter: 200, Train loss: 0.6291, LR: 0.015225\n",
      "Epoch: 6, Iter: 300, Train loss: 0.6265, LR: 0.010830\n",
      "=====================================================================\n",
      "Test set: Average loss: 0.0000, Accuracy: 8449/10000 (84.49%)\n",
      "\n",
      "=====================================================================\n",
      "Epoch: 7, Iter: 0, Train loss: 0.5576, LR: 0.007322\n",
      "Epoch: 7, Iter: 100, Train loss: 0.5889, LR: 0.004151\n",
      "Epoch: 7, Iter: 200, Train loss: 0.5376, LR: 0.001817\n",
      "Epoch: 7, Iter: 300, Train loss: 0.6439, LR: 0.000416\n",
      "=====================================================================\n",
      "Test set: Average loss: 0.0000, Accuracy: 8485/10000 (84.85%)\n",
      "\n",
      "=====================================================================\n",
      "Epoch: 8, Iter: 0, Train loss: 0.5066, LR: 0.050000\n",
      "Epoch: 8, Iter: 100, Train loss: 0.8613, LR: 0.049874\n",
      "Epoch: 8, Iter: 200, Train loss: 0.9256, LR: 0.049497\n",
      "Epoch: 8, Iter: 300, Train loss: 0.6566, LR: 0.048874\n",
      "=====================================================================\n",
      "Test set: Average loss: 0.0000, Accuracy: 8131/10000 (81.31%)\n",
      "\n",
      "=====================================================================\n",
      "Epoch: 9, Iter: 0, Train loss: 0.7845, LR: 0.048097\n",
      "Epoch: 9, Iter: 100, Train loss: 0.7246, LR: 0.047021\n",
      "Epoch: 9, Iter: 200, Train loss: 0.9799, LR: 0.045724\n",
      "Epoch: 9, Iter: 300, Train loss: 0.7038, LR: 0.044217\n",
      "=====================================================================\n",
      "Test set: Average loss: 0.0000, Accuracy: 8246/10000 (82.46%)\n",
      "\n",
      "=====================================================================\n",
      "Epoch: 10, Iter: 0, Train loss: 0.6095, LR: 0.042678\n",
      "Epoch: 10, Iter: 100, Train loss: 0.8122, LR: 0.040816\n",
      "Epoch: 10, Iter: 200, Train loss: 0.6312, LR: 0.038795\n",
      "Epoch: 10, Iter: 300, Train loss: 0.5994, LR: 0.036635\n",
      "=====================================================================\n",
      "Test set: Average loss: 0.0000, Accuracy: 8127/10000 (81.27%)\n",
      "\n",
      "=====================================================================\n",
      "Epoch: 11, Iter: 0, Train loss: 0.8174, LR: 0.034567\n",
      "Epoch: 11, Iter: 100, Train loss: 0.6403, LR: 0.032203\n",
      "Epoch: 11, Iter: 200, Train loss: 0.6413, LR: 0.029766\n",
      "Epoch: 11, Iter: 300, Train loss: 0.4473, LR: 0.027282\n",
      "=====================================================================\n",
      "Test set: Average loss: 0.0000, Accuracy: 8394/10000 (83.94%)\n",
      "\n",
      "=====================================================================\n",
      "Epoch: 12, Iter: 0, Train loss: 0.7346, LR: 0.025000\n",
      "Epoch: 12, Iter: 100, Train loss: 0.6423, LR: 0.022493\n",
      "Epoch: 12, Iter: 200, Train loss: 0.5373, LR: 0.020012\n",
      "Epoch: 12, Iter: 300, Train loss: 0.5604, LR: 0.017581\n",
      "=====================================================================\n",
      "Test set: Average loss: 0.0000, Accuracy: 8581/10000 (85.81%)\n",
      "\n",
      "=====================================================================\n",
      "Epoch: 13, Iter: 0, Train loss: 0.5507, LR: 0.015433\n",
      "Epoch: 13, Iter: 100, Train loss: 0.4646, LR: 0.013165\n",
      "Epoch: 13, Iter: 200, Train loss: 0.5664, LR: 0.011017\n",
      "Epoch: 13, Iter: 300, Train loss: 0.5335, LR: 0.009010\n",
      "=====================================================================\n",
      "Test set: Average loss: 0.0000, Accuracy: 8543/10000 (85.43%)\n",
      "\n",
      "=====================================================================\n",
      "Epoch: 14, Iter: 0, Train loss: 0.4016, LR: 0.007322\n",
      "Epoch: 14, Iter: 100, Train loss: 0.4371, LR: 0.005639\n",
      "Epoch: 14, Iter: 200, Train loss: 0.4287, LR: 0.004151\n",
      "Epoch: 14, Iter: 300, Train loss: 0.5391, LR: 0.002873\n",
      "=====================================================================\n",
      "Test set: Average loss: 0.0000, Accuracy: 8493/10000 (84.93%)\n",
      "\n",
      "=====================================================================\n",
      "Epoch: 15, Iter: 0, Train loss: 0.4135, LR: 0.001903\n",
      "Epoch: 15, Iter: 100, Train loss: 0.3870, LR: 0.001060\n",
      "Epoch: 15, Iter: 200, Train loss: 0.4387, LR: 0.000459\n",
      "Epoch: 15, Iter: 300, Train loss: 0.4659, LR: 0.000104\n",
      "=====================================================================\n",
      "Test set: Average loss: 0.0000, Accuracy: 8601/10000 (86.01%)\n",
      "\n",
      "=====================================================================\n",
      "Epoch: 16, Iter: 0, Train loss: 0.4662, LR: 0.050000\n",
      "Epoch: 16, Iter: 100, Train loss: 0.3912, LR: 0.049968\n",
      "Epoch: 16, Iter: 200, Train loss: 0.6707, LR: 0.049874\n",
      "Epoch: 16, Iter: 300, Train loss: 0.6305, LR: 0.049717\n",
      "=====================================================================\n",
      "Test set: Average loss: 0.0000, Accuracy: 8287/10000 (82.87%)\n",
      "\n",
      "=====================================================================\n",
      "Epoch: 17, Iter: 0, Train loss: 0.6571, LR: 0.049520\n",
      "Epoch: 17, Iter: 100, Train loss: 0.5439, LR: 0.049244\n",
      "Epoch: 17, Iter: 200, Train loss: 0.5824, LR: 0.048907\n",
      "Epoch: 17, Iter: 300, Train loss: 0.5827, LR: 0.048510\n",
      "=====================================================================\n",
      "Test set: Average loss: 0.0000, Accuracy: 8298/10000 (82.98%)\n",
      "\n",
      "=====================================================================\n",
      "Epoch: 18, Iter: 0, Train loss: 0.5472, LR: 0.048097\n",
      "Epoch: 18, Iter: 100, Train loss: 0.4856, LR: 0.047588\n",
      "Epoch: 18, Iter: 200, Train loss: 0.6519, LR: 0.047021\n",
      "Epoch: 18, Iter: 300, Train loss: 0.5596, LR: 0.046400\n",
      "=====================================================================\n",
      "Test set: Average loss: 0.0000, Accuracy: 8064/10000 (80.64%)\n",
      "\n",
      "=====================================================================\n",
      "Epoch: 19, Iter: 0, Train loss: 0.5695, LR: 0.045787\n",
      "Epoch: 19, Iter: 100, Train loss: 0.4612, LR: 0.045063\n",
      "Epoch: 19, Iter: 200, Train loss: 0.6669, LR: 0.044289\n",
      "Epoch: 19, Iter: 300, Train loss: 0.4572, LR: 0.043467\n",
      "=====================================================================\n",
      "Test set: Average loss: 0.0000, Accuracy: 8430/10000 (84.30%)\n",
      "\n",
      "=====================================================================\n",
      "Epoch: 20, Iter: 0, Train loss: 0.5312, LR: 0.042678\n",
      "Epoch: 20, Iter: 100, Train loss: 0.4446, LR: 0.041768\n",
      "Epoch: 20, Iter: 200, Train loss: 0.6673, LR: 0.040816\n",
      "Epoch: 20, Iter: 300, Train loss: 0.4815, LR: 0.039824\n",
      "=====================================================================\n",
      "Test set: Average loss: 0.0000, Accuracy: 8464/10000 (84.64%)\n",
      "\n",
      "=====================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21, Iter: 0, Train loss: 0.4438, LR: 0.038889\n",
      "Epoch: 21, Iter: 100, Train loss: 0.4367, LR: 0.037828\n",
      "Epoch: 21, Iter: 200, Train loss: 0.6595, LR: 0.036735\n",
      "Epoch: 21, Iter: 300, Train loss: 0.3997, LR: 0.035612\n",
      "=====================================================================\n",
      "Test set: Average loss: 0.0000, Accuracy: 8657/10000 (86.57%)\n",
      "\n",
      "=====================================================================\n",
      "Epoch: 22, Iter: 0, Train loss: 0.4773, LR: 0.034567\n",
      "Epoch: 22, Iter: 100, Train loss: 0.4977, LR: 0.033396\n",
      "Epoch: 22, Iter: 200, Train loss: 0.5119, LR: 0.032203\n",
      "Epoch: 22, Iter: 300, Train loss: 0.4052, LR: 0.030992\n",
      "=====================================================================\n",
      "Test set: Average loss: 0.0000, Accuracy: 8750/10000 (87.50%)\n",
      "\n",
      "=====================================================================\n",
      "Epoch: 23, Iter: 0, Train loss: 0.3635, LR: 0.029877\n",
      "Epoch: 23, Iter: 100, Train loss: 0.4151, LR: 0.028640\n",
      "Epoch: 23, Iter: 200, Train loss: 0.4513, LR: 0.027394\n",
      "Epoch: 23, Iter: 300, Train loss: 0.5101, LR: 0.026142\n",
      "=====================================================================\n",
      "Test set: Average loss: 0.0000, Accuracy: 8417/10000 (84.17%)\n",
      "\n",
      "=====================================================================\n",
      "Epoch: 24, Iter: 0, Train loss: 0.4717, LR: 0.025000\n",
      "Epoch: 24, Iter: 100, Train loss: 0.6089, LR: 0.023745\n",
      "Epoch: 24, Iter: 200, Train loss: 0.4937, LR: 0.022493\n",
      "Epoch: 24, Iter: 300, Train loss: 0.3494, LR: 0.021248\n",
      "=====================================================================\n",
      "Test set: Average loss: 0.0000, Accuracy: 8807/10000 (88.07%)\n",
      "\n",
      "=====================================================================\n",
      "Epoch: 25, Iter: 0, Train loss: 0.3481, LR: 0.020123\n",
      "Epoch: 25, Iter: 100, Train loss: 0.4226, LR: 0.018898\n",
      "Epoch: 25, Iter: 200, Train loss: 0.4529, LR: 0.017689\n",
      "Epoch: 25, Iter: 300, Train loss: 0.4813, LR: 0.016498\n",
      "=====================================================================\n",
      "Test set: Average loss: 0.0000, Accuracy: 8663/10000 (86.63%)\n",
      "\n",
      "=====================================================================\n",
      "Epoch: 26, Iter: 0, Train loss: 0.4091, LR: 0.015433\n",
      "Epoch: 26, Iter: 100, Train loss: 0.4526, LR: 0.014286\n",
      "Epoch: 26, Iter: 200, Train loss: 0.3230, LR: 0.013165\n",
      "Epoch: 26, Iter: 300, Train loss: 0.3537, LR: 0.012075\n",
      "=====================================================================\n",
      "Test set: Average loss: 0.0000, Accuracy: 8782/10000 (87.82%)\n",
      "\n",
      "=====================================================================\n",
      "Epoch: 27, Iter: 0, Train loss: 0.3571, LR: 0.011111\n",
      "Epoch: 27, Iter: 100, Train loss: 0.3866, LR: 0.010085\n",
      "Epoch: 27, Iter: 200, Train loss: 0.3480, LR: 0.009097\n",
      "Epoch: 27, Iter: 300, Train loss: 0.6376, LR: 0.008148\n",
      "=====================================================================\n",
      "Test set: Average loss: 0.0000, Accuracy: 8633/10000 (86.33%)\n",
      "\n",
      "=====================================================================\n",
      "Epoch: 28, Iter: 0, Train loss: 0.3338, LR: 0.007322\n",
      "Epoch: 28, Iter: 100, Train loss: 0.3763, LR: 0.006457\n",
      "Epoch: 28, Iter: 200, Train loss: 0.3226, LR: 0.005639\n",
      "Epoch: 28, Iter: 300, Train loss: 0.3014, LR: 0.004869\n",
      "=====================================================================\n",
      "Test set: Average loss: 0.0000, Accuracy: 8793/10000 (87.93%)\n",
      "\n",
      "=====================================================================\n",
      "Epoch: 29, Iter: 0, Train loss: 0.3099, LR: 0.004213\n",
      "Epoch: 29, Iter: 100, Train loss: 0.3464, LR: 0.003542\n",
      "Epoch: 29, Iter: 200, Train loss: 0.4138, LR: 0.002925\n",
      "Epoch: 29, Iter: 300, Train loss: 0.3813, LR: 0.002364\n",
      "=====================================================================\n",
      "Test set: Average loss: 0.0000, Accuracy: 8916/10000 (89.16%)\n",
      "\n",
      "=====================================================================\n",
      "Epoch: 30, Iter: 0, Train loss: 0.2469, LR: 0.001903\n",
      "Epoch: 30, Iter: 100, Train loss: 0.4254, LR: 0.001452\n",
      "Epoch: 30, Iter: 200, Train loss: 0.3598, LR: 0.001060\n",
      "Epoch: 30, Iter: 300, Train loss: 0.3354, LR: 0.000729\n",
      "=====================================================================\n",
      "Test set: Average loss: 0.0000, Accuracy: 8817/10000 (88.17%)\n",
      "\n",
      "=====================================================================\n",
      "Epoch: 31, Iter: 0, Train loss: 0.1717, LR: 0.000480\n",
      "Epoch: 31, Iter: 100, Train loss: 0.3960, LR: 0.000266\n",
      "Epoch: 31, Iter: 200, Train loss: 0.2259, LR: 0.000115\n",
      "Epoch: 31, Iter: 300, Train loss: 0.2662, LR: 0.000026\n",
      "=====================================================================\n",
      "Test set: Average loss: 0.0000, Accuracy: 8781/10000 (87.81%)\n",
      "\n",
      "=====================================================================\n",
      "Epoch: 32, Iter: 0, Train loss: 0.3945, LR: 0.050000\n",
      "Epoch: 32, Iter: 100, Train loss: 0.4336, LR: 0.049992\n",
      "Epoch: 32, Iter: 200, Train loss: 0.5103, LR: 0.049968\n",
      "Epoch: 32, Iter: 300, Train loss: 0.6329, LR: 0.049929\n",
      "=====================================================================\n",
      "Test set: Average loss: 0.0000, Accuracy: 8215/10000 (82.15%)\n",
      "\n",
      "=====================================================================\n",
      "Epoch: 33, Iter: 0, Train loss: 0.4804, LR: 0.049880\n",
      "Epoch: 33, Iter: 100, Train loss: 0.5708, LR: 0.049810\n",
      "Epoch: 33, Iter: 200, Train loss: 0.4888, LR: 0.049725\n",
      "Epoch: 33, Iter: 300, Train loss: 0.5902, LR: 0.049625\n",
      "=====================================================================\n",
      "Test set: Average loss: 0.0000, Accuracy: 8465/10000 (84.65%)\n",
      "\n",
      "=====================================================================\n",
      "Epoch: 34, Iter: 0, Train loss: 0.6436, LR: 0.049520\n",
      "Epoch: 34, Iter: 100, Train loss: 0.4366, LR: 0.049389\n",
      "Epoch: 34, Iter: 200, Train loss: 0.3993, LR: 0.049244\n",
      "Epoch: 34, Iter: 300, Train loss: 0.4795, LR: 0.049083\n",
      "=====================================================================\n",
      "Test set: Average loss: 0.0000, Accuracy: 5209/10000 (52.09%)\n",
      "\n",
      "=====================================================================\n",
      "Epoch: 35, Iter: 0, Train loss: 1.4381, LR: 0.048924\n",
      "Epoch: 35, Iter: 100, Train loss: 0.7688, LR: 0.048734\n",
      "Epoch: 35, Iter: 200, Train loss: 0.4505, LR: 0.048529\n",
      "Epoch: 35, Iter: 300, Train loss: 0.7066, LR: 0.048310\n",
      "=====================================================================\n",
      "Test set: Average loss: 0.0000, Accuracy: 8413/10000 (84.13%)\n",
      "\n",
      "=====================================================================\n",
      "Epoch: 36, Iter: 0, Train loss: 0.4955, LR: 0.048097\n",
      "Epoch: 36, Iter: 100, Train loss: 0.5152, LR: 0.047850\n",
      "Epoch: 36, Iter: 200, Train loss: 0.5262, LR: 0.047588\n",
      "Epoch: 36, Iter: 300, Train loss: 0.4633, LR: 0.047312\n",
      "=====================================================================\n",
      "Test set: Average loss: 0.0000, Accuracy: 8507/10000 (85.07%)\n",
      "\n",
      "=====================================================================\n",
      "Epoch: 37, Iter: 0, Train loss: 0.5285, LR: 0.047048\n",
      "Epoch: 37, Iter: 100, Train loss: 0.4892, LR: 0.046745\n",
      "Epoch: 37, Iter: 200, Train loss: 0.4333, LR: 0.046429\n",
      "Epoch: 37, Iter: 300, Train loss: 0.5790, LR: 0.046099\n",
      "=====================================================================\n",
      "Test set: Average loss: 0.0000, Accuracy: 8536/10000 (85.36%)\n",
      "\n",
      "=====================================================================\n",
      "Epoch: 38, Iter: 0, Train loss: 0.4797, LR: 0.045787\n",
      "Epoch: 38, Iter: 100, Train loss: 0.5438, LR: 0.045431\n",
      "Epoch: 38, Iter: 200, Train loss: 0.5896, LR: 0.045063\n",
      "Epoch: 38, Iter: 300, Train loss: 0.6472, LR: 0.044683\n",
      "=====================================================================\n",
      "Test set: Average loss: 0.0000, Accuracy: 8503/10000 (85.03%)\n",
      "\n",
      "=====================================================================\n",
      "Epoch: 39, Iter: 0, Train loss: 0.4508, LR: 0.044325\n",
      "Epoch: 39, Iter: 100, Train loss: 0.4264, LR: 0.043921\n",
      "Epoch: 39, Iter: 200, Train loss: 0.3688, LR: 0.043505\n",
      "Epoch: 39, Iter: 300, Train loss: 0.4994, LR: 0.043077\n",
      "=====================================================================\n",
      "Test set: Average loss: 0.0000, Accuracy: 8609/10000 (86.09%)\n",
      "\n",
      "=====================================================================\n",
      "Epoch: 40, Iter: 0, Train loss: 0.4398, LR: 0.042678\n",
      "Epoch: 40, Iter: 100, Train loss: 0.4284, LR: 0.042228\n",
      "Epoch: 40, Iter: 200, Train loss: 0.3613, LR: 0.041768\n",
      "Epoch: 40, Iter: 300, Train loss: 0.2768, LR: 0.041297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================================\n",
      "Test set: Average loss: 0.0000, Accuracy: 7810/10000 (78.10%)\n",
      "\n",
      "=====================================================================\n",
      "Epoch: 41, Iter: 0, Train loss: 0.7693, LR: 0.040860\n",
      "Epoch: 41, Iter: 100, Train loss: 0.4706, LR: 0.040370\n",
      "Epoch: 41, Iter: 200, Train loss: 0.4512, LR: 0.039870\n",
      "Epoch: 41, Iter: 300, Train loss: 0.5398, LR: 0.039361\n",
      "=====================================================================\n",
      "Test set: Average loss: 0.0000, Accuracy: 8543/10000 (85.43%)\n",
      "\n",
      "=====================================================================\n",
      "Epoch: 42, Iter: 0, Train loss: 0.5127, LR: 0.038889\n",
      "Epoch: 42, Iter: 100, Train loss: 0.5497, LR: 0.038363\n",
      "Epoch: 42, Iter: 200, Train loss: 0.3968, LR: 0.037828\n",
      "Epoch: 42, Iter: 300, Train loss: 0.3835, LR: 0.037286\n",
      "=====================================================================\n",
      "Test set: Average loss: 0.0000, Accuracy: 8729/10000 (87.29%)\n",
      "\n",
      "=====================================================================\n",
      "Epoch: 43, Iter: 0, Train loss: 0.4909, LR: 0.036785\n"
     ]
    }
   ],
   "source": [
    "model = BridgeNet(pretrained=True, route='cifar10')\n",
    "model.init_weight('cifar10')\n",
    "model = nn.DataParallel(model)\n",
    "\n",
    "loss_func = nn.NLLLoss()\n",
    "\n",
    "train(model, cifar10_train_loader, cifar10_test_loader, loss_func, num_epochs=127)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
